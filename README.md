# RL Hyperparameter Showdown: MountainCar vs CartPole 

**Comprehensive comparison of Q-Learning hyperparameters across two classic Gymnasium environments**

This repository contains two sister projects that systematically test Q-Learning configurations to reveal how **hyperparameters dramatically change performance** based on environment characteristics.

## What I'm Achieving

**Core Question**: *Why do the same hyperparameters fail spectacularly in one environment but excel in another?*

**Methodology**:
- Identical Q-Learning implementation
- Same 7 configurations tested
- 10,000 episodes per experiment
- Rich visualization + logging
- **Cross-environment comparison**

**Key Insight**: **No universal "best" hyperparameters** â€” success depends on environment dynamics!

## Environment Showdown

| Feature | **MountainCar-v0** | **CartPole-v1** |
|---------|----------------------|-------------------|
| **Reward Structure** | -1/step, sparse (goal only) | +1/step, dense |
| **Success Metric** | Reach flag (~110 steps) | Balance 500 steps |
| **State Space** | 2D continuous | 4D continuous |
| **Core Challenge** | **Exploration** + momentum | **Precision** control |
| **Planning Horizon** | Long-term (swing strategy) | Short-term corrections |

## Winners & Losers

| Config | **MountainCar** | **CartPole** | **Why?** |
|--------|-------------------|----------------|----------|
| `slow_exploration` | **-146.8** | ~248.3 | Long exploration = momentum building |
| `fine_discretization` | ~164.7 | **258.0** | Precision = better control |
| `baseline` | ~177.6 | ~160.1 | Works okay everywhere |
| `high_discount` | ~160.7 | - | Values future rewards |
| `fast_exploration` | ~189.7 | ~157.4 | Locks into bad policies |
| `high_learning_rate` | ~196.8 | **13.2** | Unstable everywhere |
| `low_discount` | **~199.6** | ~144.0 | Ignores future rewards |

## Deep Insights

### 1. **Exploration Speed Kills MountainCar**

MountainCar needs: LEFT â†’ momentum â†’ RIGHT
fast_exploration: "Exploits early" â†’ stuck
slow_exploration: "Explores long enough" â†’ learns swing

### 2. **Precision Wins CartPole**

CartPole needs: tiny corrections every step
coarse buckets: "Misses small angle changes"
fine_discretization: "Sees subtle deviations" â†’ 500-step mastery


### 3. **Learning Rate Trap**

Î± = 0.5 destroys both environments:

Overshoots optimal Q-values

Creates instability oscillations

Reward collapse inevitable

## Hyperparameter Decision Tree

```mermaid
graph TD
    A[Environment Type?] -->|Sparse/Long-horizon| B[Slow Îµ decay<br/>High Î³]
    A -->|Dense/Control| C[Fine discretization<br/>Moderate Îµ]
    B --> D[MountainCar: slow_exploration]
    C --> E[CartPole: fine_discretization]
```

### Repository Structure

```
Reinforcement Learning/
â”œâ”€â”€ README.md                 â† This file
â”œâ”€â”€ mountaincar_qlearning/
â”‚   â”œâ”€â”€ README.md            â† Detailed MountainCar analysis
â”‚   â”œâ”€â”€ mountaincar_qlearning.py
â”‚   â””â”€â”€ experiments/
â””â”€â”€ cartpole_qlearning/
    â”œâ”€â”€ README.md            â† Detailed CartPole analysis
    â”œâ”€â”€ cartpole_qlearning.py
    â””â”€â”€ experiments/
```
---
## All Results Heatmap

| Config | ğŸ”ï¸ **MountainCar**<br/>(â†‘ better) | ğŸ¯ **CartPole**<br/>(â†‘ better) | **Robustness** |
|--------|----------------------------------|-------------------------------|---------------|
| `slow_exploration` | **-146.8** ğŸ¥‡ | **248.3** ğŸ¥ˆ | â­â­â­â­â­ |
| `fine_discretization` | -164.7 ğŸ¥ˆ | **258.0** ğŸ¥‡ | â­â­â­â­ |
| `baseline` | -177.6 | 160.1 | â­â­â­ |
| `high_discount` | -160.7 ğŸ¥‰ | - | â­â­â­ |
| `fast_exploration` | -189.7 | 157.4 | â­â­ |
| `high_learning_rate` | -196.8 | **13.2** | â­ |
| `low_discount` | **-199.6** | 144.0 | â­ |

## How to Make It Even Better
### Immediate Improvements
âœ… Add DQN (neural Q-learning) baseline

âœ… Test different bucket strategies

âœ… Include Double Q-Learning

âœ… Add SARSA comparison

âœ… Hyperparameter optimization (Optuna)

### Future Extensions
ğŸ”„ Continuous action spaces (PPO/DQN)

ğŸ”„ Multi-agent scenarios

ğŸ”„ Transfer learning between envs

ğŸ”„ Real-world robotics deployment

### Takeaways for Your RL Journey
Test systematically â€” don't guess hyperparameters

Understand environment dynamics â€” sparse vs dense rewards

Visualize everything â€” curves reveal hidden behaviors

Never use Î± > 0.1 â€” stability first

Match exploration to problem â€” slow for sparse, moderate for control